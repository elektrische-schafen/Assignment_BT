{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Sf4yJObOa4FE",
        "outputId": "643a77a9-7f8a-437b-ac28-aa143150f978"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b82a5c7-db29-437f-9cc0-7d96fe9cddc0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b82a5c7-db29-437f-9cc0-7d96fe9cddc0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n",
            "Using train file: train.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # pick your train.csv\n",
        "train_path = next(iter(uploaded.keys()))\n",
        "print(\"Using train file:\", train_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()  # pick your test.csv\n",
        "test_path = next(iter(uploaded.keys()))\n",
        "print(\"Using test file:\", test_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "WZMEsNpna8cF",
        "outputId": "2e13bdd7-43df-4072-a587-8354b3485d57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e0c051f7-032b-405a-810f-face71cd6e32\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e0c051f7-032b-405a-810f-face71cd6e32\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test.csv\n",
            "Using test file: test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# 1. Preprocessing\n",
        "# ==========================\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Basic text cleaning: lowercase, remove weird chars, normalize spaces.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\"\n",
        "    text = text.lower()\n",
        "    # keep letters, digits, and common punctuation\n",
        "    text = re.sub(r\"[^a-z0-9'?!.:, ]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# 2. Data loading\n",
        "# ==========================\n",
        "\n",
        "def load_train_data(path: str = \"train.csv\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load train.csv and create a 'combined_text' column for retrieval.\n",
        "    IMPORTANT CHANGE: Use ONLY Question text for retrieval to reduce bias\n",
        "    toward generic answers.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    required_cols = {\"Question\", \"Answer\"}\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        raise ValueError(f\"train.csv must contain columns: {required_cols}\")\n",
        "\n",
        "    df[\"Question\"] = df[\"Question\"].fillna(\"\")\n",
        "    df[\"Answer\"] = df[\"Answer\"].fillna(\"\")\n",
        "\n",
        "    # Use only the question text for retrieval\n",
        "    df[\"combined_text\"] = df[\"Question\"].apply(preprocess)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# 3. Base retrieval chatbot\n",
        "# ==========================\n",
        "\n",
        "class RetrievalChatbot:\n",
        "    \"\"\"\n",
        "    A retrieval-based chatbot that:\n",
        "    - learns from train.csv Q/A pairs\n",
        "    - retrieves the most similar past example using TFâ€“IDF + cosine similarity\n",
        "    - avoids repeating very recent answers for a given user\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        min_sim: float = 0.15,\n",
        "        max_candidates: int = 100,\n",
        "        max_history_turns: int = 3,\n",
        "        repeat_window: int = 5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        train_df must have:\n",
        "        - Question\n",
        "        - Answer\n",
        "        - category (optional)\n",
        "        - combined_text (preprocessed Question)\n",
        "\n",
        "        min_sim: minimum similarity to trust retrieval. Below this â†’ fallback.\n",
        "        max_candidates: how many top candidates to consider before choosing.\n",
        "        max_history_turns: number of past turns used as short-term context.\n",
        "        repeat_window: how many last bot answers to avoid repeating.\n",
        "        \"\"\"\n",
        "        self.train_df = train_df.reset_index(drop=True)\n",
        "        self.min_sim = min_sim\n",
        "        self.max_candidates = max_candidates\n",
        "        self.max_history_turns = max_history_turns\n",
        "        self.repeat_window = repeat_window\n",
        "\n",
        "        # Vectorizer for retrieval (on combined_text = Question-only)\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words=\"english\",\n",
        "            min_df=2,\n",
        "            max_df=0.95,\n",
        "        )\n",
        "        self.X_train = self.vectorizer.fit_transform(self.train_df[\"combined_text\"])\n",
        "\n",
        "        # history: dict[user_id] = list of dicts: {\"user\": str, \"bot\": str}\n",
        "        self.histories: Dict[str, List[Dict[str, str]]] = {}\n",
        "\n",
        "    # ---------- history helpers ----------\n",
        "\n",
        "    def _get_history(self, user_id: str) -> List[Dict[str, str]]:\n",
        "        return self.histories.get(user_id, [])\n",
        "\n",
        "    def _append_history(self, user_id: str, user_msg: str, bot_msg: str) -> None:\n",
        "        hist = self.histories.setdefault(user_id, [])\n",
        "        hist.append({\"user\": user_msg, \"bot\": bot_msg})\n",
        "\n",
        "    def _build_context(self, user_id: str) -> str:\n",
        "        \"\"\"\n",
        "        Turn last few exchanges into a short context string.\n",
        "        \"\"\"\n",
        "        recent = self._get_history(user_id)[-self.max_history_turns :]\n",
        "        pieces = []\n",
        "        for turn in recent:\n",
        "            pieces.append(f\"user: {turn['user']}\")\n",
        "            pieces.append(f\"bot: {turn['bot']}\")\n",
        "        return \" \".join(pieces)\n",
        "\n",
        "    def _recent_answers_set(self, user_id: str) -> set:\n",
        "        \"\"\"\n",
        "        Collect last few bot answers to avoid repeating them.\n",
        "        \"\"\"\n",
        "        hist = self._get_history(user_id)\n",
        "        recent = hist[-self.repeat_window :]\n",
        "        return {turn[\"bot\"] for turn in recent}\n",
        "\n",
        "    # ---------- generic replies ----------\n",
        "\n",
        "    def _fallback_response(self, user_input: str) -> str:\n",
        "        \"\"\"\n",
        "        Generic replies when similarity is too low.\n",
        "        \"\"\"\n",
        "        generic_replies = [\n",
        "            \"I'm not completely sure how to answer that, but I'm listening.\",\n",
        "            \"Interesting! Tell me more about that.\",\n",
        "            \"That's a good question. What do you think?\",\n",
        "            \"I'm not sure, but I'm happy to chat about it!\",\n",
        "        ]\n",
        "        return random.choice(generic_replies)\n",
        "\n",
        "    # ---------- retrieval core ----------\n",
        "\n",
        "    def _retrieve_best_match(\n",
        "        self,\n",
        "        contextualized_input: str,\n",
        "        user_id: str,\n",
        "    ) -> Tuple[str, Optional[str], float]:\n",
        "        \"\"\"\n",
        "        Retrieve a good, non-recent answer from train_df.\n",
        "        Returns (answer, category, best_similarity).\n",
        "        \"\"\"\n",
        "        processed = preprocess(contextualized_input)\n",
        "        if not processed:\n",
        "            return \"Could you rephrase that?\", None, 0.0\n",
        "\n",
        "        user_vec = self.vectorizer.transform([processed])\n",
        "        sims = cosine_similarity(user_vec, self.X_train)[0]\n",
        "\n",
        "        # sort indices by similarity (descending)\n",
        "        ranked_indices = sims.argsort()[::-1]\n",
        "\n",
        "        # consider only the top max_candidates\n",
        "        ranked_indices = ranked_indices[: self.max_candidates]\n",
        "\n",
        "        recent_answers = self._recent_answers_set(user_id)\n",
        "        chosen_idx = None\n",
        "        chosen_sim = 0.0\n",
        "\n",
        "        # Choose first highly similar answer that isn't recently used\n",
        "        for idx in ranked_indices:\n",
        "            sim = float(sims[idx])\n",
        "            candidate_answer = self.train_df.iloc[idx][\"Answer\"]\n",
        "\n",
        "            if sim < self.min_sim:\n",
        "                # beyond this point, all sims will be lower\n",
        "                break\n",
        "\n",
        "            if candidate_answer in recent_answers:\n",
        "                # skip recently used answers\n",
        "                continue\n",
        "\n",
        "            chosen_idx = int(idx)\n",
        "            chosen_sim = sim\n",
        "            break\n",
        "\n",
        "        # If we didn't find a non-recent candidate above min_sim,\n",
        "        # fall back to the single best match (even if repeated),\n",
        "        # so we don't always fallback to generic.\n",
        "        if chosen_idx is None:\n",
        "            best_idx = int(ranked_indices[0])\n",
        "            chosen_idx = best_idx\n",
        "            chosen_sim = float(sims[best_idx])\n",
        "\n",
        "        row = self.train_df.iloc[chosen_idx]\n",
        "        answer = row[\"Answer\"]\n",
        "        category = row.get(\"category\", None)\n",
        "\n",
        "        return answer, category, chosen_sim\n",
        "\n",
        "    def respond(\n",
        "        self,\n",
        "        user_input: str,\n",
        "        user_id: str = \"default\",\n",
        "    ) -> Tuple[str, Optional[str], float]:\n",
        "        \"\"\"\n",
        "        Main response function.\n",
        "        Uses short-term conversation context + retrieval + repetition control.\n",
        "        \"\"\"\n",
        "        context = self._build_context(user_id)\n",
        "        if context:\n",
        "            contextualized_input = context + \" user: \" + user_input\n",
        "        else:\n",
        "            contextualized_input = user_input\n",
        "\n",
        "        answer, category, sim = self._retrieve_best_match(contextualized_input, user_id)\n",
        "\n",
        "        if sim < self.min_sim:\n",
        "            # If even our chosen candidate is low-sim, go generic\n",
        "            answer = self._fallback_response(user_input)\n",
        "            category = None\n",
        "\n",
        "        self._append_history(user_id, user_input, answer)\n",
        "        return answer, category, sim\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# 4. Personalization: user profiles & tone\n",
        "# ==========================\n",
        "\n",
        "class PersonalizedRetrievalChatbot(RetrievalChatbot):\n",
        "    \"\"\"\n",
        "    Extension of RetrievalChatbot with:\n",
        "    - long-term user profiles stored in JSON\n",
        "    - tone personalization (friendly / enthusiastic / formal / witty)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        profile_path: str = \"profiles.json\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(train_df, **kwargs)\n",
        "        self.profile_path = Path(profile_path)\n",
        "        self.user_profiles: Dict[str, Dict] = self._load_profiles()\n",
        "\n",
        "    # ---------- profiles ----------\n",
        "\n",
        "    def _load_profiles(self) -> Dict[str, Dict]:\n",
        "        if self.profile_path.exists():\n",
        "            try:\n",
        "                return json.loads(self.profile_path.read_text(encoding=\"utf-8\"))\n",
        "            except Exception:\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def _save_profiles(self) -> None:\n",
        "        self.profile_path.write_text(\n",
        "            json.dumps(self.user_profiles, indent=2), encoding=\"utf-8\"\n",
        "        )\n",
        "\n",
        "    def get_profile(self, user_id: str) -> Dict:\n",
        "        return self.user_profiles.get(user_id, {})\n",
        "\n",
        "    def update_profile(self, user_id: str, **updates) -> None:\n",
        "        profile = self.user_profiles.get(user_id, {})\n",
        "        profile.update(updates)\n",
        "        self.user_profiles[user_id] = profile\n",
        "        self._save_profiles()\n",
        "\n",
        "    # ---------- tone shaping ----------\n",
        "\n",
        "    def _apply_tone(self, answer: str, tone: Optional[str]) -> str:\n",
        "        \"\"\"\n",
        "        Simple tone adjustment.\n",
        "        \"\"\"\n",
        "        if not tone:\n",
        "            return answer\n",
        "\n",
        "        tone = tone.lower()\n",
        "        if tone == \"friendly\":\n",
        "            return answer + \" ðŸ˜Š\"\n",
        "        elif tone == \"enthusiastic\":\n",
        "            return answer + \" ðŸŽ‰\"\n",
        "        elif tone == \"formal\":\n",
        "            if not answer.lower().startswith((\"certainly\", \"of course\", \"indeed\")):\n",
        "                return \"Certainly. \" + answer\n",
        "            return answer\n",
        "        elif tone == \"witty\":\n",
        "            extras = [\n",
        "                \" (At least that's what my circuits think.)\",\n",
        "                \" (Spoken like a true chatbot philosopher.)\",\n",
        "                \" I promise I didn't just make that up. Well, maybe a little.\",\n",
        "            ]\n",
        "            return answer + random.choice(extras)\n",
        "        else:\n",
        "            return answer\n",
        "\n",
        "    # ---------- respond override ----------\n",
        "\n",
        "    def respond(\n",
        "        self,\n",
        "        user_input: str,\n",
        "        user_id: str = \"default\",\n",
        "    ) -> Tuple[str, Optional[str], float]:\n",
        "        \"\"\"\n",
        "        Personalized response:\n",
        "        - uses history/context + retrieval + repetition control\n",
        "        - then adjusts tone based on user profile\n",
        "        \"\"\"\n",
        "        profile = self.get_profile(user_id)\n",
        "        preferred_tone = profile.get(\"preferred_tone\")\n",
        "\n",
        "        answer, category, sim = super().respond(user_input, user_id=user_id)\n",
        "\n",
        "        answer = self._apply_tone(answer, preferred_tone)\n",
        "\n",
        "        # Replace last history bot text with toned version\n",
        "        history = self._get_history(user_id)\n",
        "        if history:\n",
        "            history[-1][\"bot\"] = answer\n",
        "\n",
        "        return answer, category, sim\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# 5. Simple CLI chat loop\n",
        "# ==========================\n",
        "\n",
        "def chat_loop():\n",
        "    print(\"Loading training data...\")\n",
        "    train_df = load_train_data(\"train.csv\")\n",
        "\n",
        "    print(\"Building personalized retrieval chatbot...\")\n",
        "    bot = PersonalizedRetrievalChatbot(\n",
        "        train_df,\n",
        "        profile_path=\"profiles.json\",\n",
        "        min_sim=0.15,\n",
        "        max_candidates=100,\n",
        "        max_history_turns=3,\n",
        "        repeat_window=5,\n",
        "    )\n",
        "\n",
        "    # --- user onboarding / personalization ---\n",
        "    print(\"\\nWelcome! Let's personalize your experience a bit.\")\n",
        "    user_id = input(\"Choose a username (or press Enter for 'default'): \").strip()\n",
        "    if not user_id:\n",
        "        user_id = \"default\"\n",
        "\n",
        "    profile = bot.get_profile(user_id)\n",
        "    if profile:\n",
        "        print(f\"\\nWelcome back, {profile.get('name', user_id)}!\")\n",
        "        if \"preferred_tone\" in profile:\n",
        "            print(f\"Your current preferred tone is: {profile['preferred_tone']}\")\n",
        "    else:\n",
        "        # New profile\n",
        "        name = input(\"What should I call you? (name or nickname): \").strip()\n",
        "        if not name:\n",
        "            name = user_id\n",
        "        print(\"\\nHow would you like me to talk to you?\")\n",
        "        print(\"Options: friendly / enthusiastic / formal / witty (or leave blank)\")\n",
        "        tone = input(\"Preferred tone: \").strip().lower() or None\n",
        "\n",
        "        bot.update_profile(user_id, name=name, preferred_tone=tone)\n",
        "        print(f\"\\nNice to meet you, {name}!\")\n",
        "\n",
        "    print(\"\\nChatbot is ready! Type 'exit' or 'quit' to stop.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # --- main loop ---\n",
        "    while True:\n",
        "        user_input = input(f\"{user_id}: \").strip()\n",
        "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "            print(\"Bot: Goodbye! ðŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        answer, category, sim = bot.respond(user_input, user_id=user_id)\n",
        "\n",
        "        if category:\n",
        "            print(f\"Bot ({category}, sim={sim:.2f}): {answer}\")\n",
        "        else:\n",
        "            print(f\"Bot: {answer}\")"
      ],
      "metadata": {
        "id": "S8Jvb0J7ccrR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chat_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBuLfKAmbAwq",
        "outputId": "43f6e006-0377-4daf-93c2-a4654efdc79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Building personalized retrieval chatbot...\n",
            "\n",
            "Welcome! Let's personalize your experience a bit.\n",
            "Choose a username (or press Enter for 'default'): John\n",
            "What should I call you? (name or nickname): Johnny\n",
            "\n",
            "How would you like me to talk to you?\n",
            "Options: friendly / enthusiastic / formal / witty (or leave blank)\n",
            "Preferred tone: witty\n",
            "\n",
            "Nice to meet you, Johnny!\n",
            "\n",
            "Chatbot is ready! Type 'exit' or 'quit' to stop.\n",
            "--------------------------------------------------\n",
            "John: Hi!\n",
            "Bot (caring, sim=1.00): Hello there! (Spoken like a true chatbot philosopher.)\n",
            "John: How was your day?\n",
            "Bot (professional, sim=0.64): Hello. (At least that's what my circuits think.)\n",
            "John: How old are you?\n",
            "Bot (witty, sim=0.57): Hey. (Spoken like a true chatbot philosopher.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRksCFmpbMW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}