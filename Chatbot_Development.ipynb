{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ia-JDdpXUZ4R",
        "outputId": "893938d9-ae85-425e-e886-0a57f0964aef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6b47a166-67b4-4e71-ae81-cf9c677385d8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6b47a166-67b4-4e71-ae81-cf9c677385d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n",
            "Using train file: train.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # pick your train.csv\n",
        "train_path = next(iter(uploaded.keys()))\n",
        "print(\"Using train file:\", train_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()  # pick your test.csv\n",
        "test_path = next(iter(uploaded.keys()))\n",
        "print(\"Using test file:\", test_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "_jlCQIvKUukf",
        "outputId": "d8d3840b-c871-4296-afcb-082ad9f160b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d6536745-d7eb-42ae-8a69-fef0d78ab2fa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d6536745-d7eb-42ae-8a69-fef0d78ab2fa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test.csv\n",
            "Using test file: test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall idea\n",
        "We already have a dataset (`train.csv`) containing:\n",
        "\n",
        "\n",
        "*   A `Question` (what a user might say).\n",
        "*   An `Answer` (how the bot could respond).\n",
        "\n",
        "\n",
        "*   A `category` (type of response, like ‚Äúwitty‚Äù, ‚Äúfriendly‚Äù, etc.).\n",
        "The chatbot does **not generate new sentences from scratch**. Instead, when the user types something:\n",
        "\n",
        "\n",
        "*   t converts that text into a vector using **TF‚ÄìIDF**.\n",
        "*   It compares this vector to all vectors of training `combined_text` (Question + Answer).\n",
        "\n",
        "\n",
        "*   It finds the **most similar** training example **using cosine similarity**.\n",
        "*   It returns the **stored Answer** from that example.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "re5Vxrg_dohS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random"
      ],
      "metadata": {
        "id": "KhuTi_rre4Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The `preprocess` function\n",
        "What it does, step by step:\n",
        "\n",
        "\n",
        "*   Type check: If `text` is not a string (maybe `NaN` or `None` from the CSV), we replace it with empty string `\"\"` to avoid errors.\n",
        "*   Lowercasing: This makes the matching case-insensitive: ‚ÄúHello‚Äù and ‚Äúhello‚Äù are treated the same.\n",
        "\n",
        "\n",
        "*   Remove unwanted characters: Uses `re.sub` (regular expression) to remove characters that are not: **a)** lowercase letters, **b)** digits **c)** some punctuation commonly used in chat, **d)** everything else (emojis, weird symbols, etc.) is replaced with a space\n",
        "*   Normalize whitespace: a) `re.sub(r\"\\s+\", \" \", text)` collapses multiple spaces (or tabs/newlines) into a single space. b) `re.sub(r\"\\s+\", \" \", text)` collapses multiple spaces (or tabs/newlines) into a single space.\n",
        "\n",
        "Result: a clean, normalized string that‚Äôs consistent for vectorization.\n",
        "\n"
      ],
      "metadata": {
        "id": "PXWdNSZ2e8Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1. Preprocessing ----------\n",
        "def preprocess(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Basic text cleaning: lowercase, remove weird chars, normalize spaces.\n",
        "    Adjust this if you want more sophisticated preprocessing.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\"\n",
        "    text = text.lower()\n",
        "    # keep letters, digits, punctuation common in chat\n",
        "    text = re.sub(r\"[^a-z0-9'?!.:, ]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "8Rsc-2GAe5II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Data: `load_data`\n",
        "* R ead the CSV:\n",
        "  * df = pd.read_csv(train_path) loads train.csv into a pandas DataFrame.\n",
        "  * This DataFrame has at least `Question`, `Answer`, `category`.\n",
        "\n",
        "* Create combined_text\n",
        "  * df[\"Question\"].fillna(\"\") replaces missing questions with an empty string.\n",
        "\n",
        "  * df[\"Answer\"].fillna(\"\") does the same for answers.\n",
        "\n",
        "  * They‚Äôre concatenated with a space in between: Question + \" \" + Answer.\n",
        "\n",
        "  * Why? Because we want the retrieval model to look at both the question and answer together when learning similarity. That way, if the user asks something like the original question, or something that resembles that Q/A pair, we can match it.\n",
        "\n",
        "* Preprocess combined text\n",
        "\n",
        "  * `.apply(preprocess)` runs the cleaning function over each combined string.\n",
        "\n",
        "  * The result is stored in df[\"combined_text\"].\n",
        "\n",
        "So `load_data` returns a DataFrame where each row has:\n",
        "\n",
        "* `Question`\n",
        "\n",
        "* `Answer`\n",
        "\n",
        "* `category`\n",
        "\n",
        "* `combined_text` (cleaned ‚ÄúQuestion + Answer‚Äù string)"
      ],
      "metadata": {
        "id": "19rYbm3Lgat4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 2. Load data ----------\n",
        "def load_data(train_path: str = \"train.csv\"):\n",
        "    df = pd.read_csv(train_path)\n",
        "\n",
        "    # Combine question + answer into a single text field for retrieval\n",
        "    df[\"combined_text\"] = (\n",
        "        df[\"Question\"].fillna(\"\") + \" \" + df[\"Answer\"].fillna(\"\")\n",
        "    ).apply(preprocess)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "NvuvwBHCgZ95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `RetrievalChatbot` Class\n",
        "\n",
        "What it sets up:\n",
        "\n",
        "* Store training data\n",
        "\n",
        "  * `self.train_df = train_df.reset_index(drop=True)`\n",
        "\n",
        "  * `reset_index(drop=True)` ensures a clean 0..N-1 index.\n",
        "\n",
        "* Hyperparameters\n",
        "\n",
        "  * `min_sim`: If the best similarity is below this threshold, we consider the match ‚Äútoo weak‚Äù and instead use a generic fallback response.\n",
        "\n",
        "  * `top_k`: Instead of always choosing the single best match, we pick randomly among the top k matches. This introduces slight variety in answers.\n",
        "\n",
        "* TF‚ÄìIDF Vectorizer\n",
        "\n",
        "  * `self.vectorizer = TfidfVectorizer(ngram_range=(1, 2))`\n",
        "\n",
        "  * TF‚ÄìIDF stands for Term Frequency ‚Äì Inverse Document Frequency.\n",
        "\n",
        "  * `ngram_range=(1, 2)` means we consider:\n",
        "\n",
        "    * unigrams: single words like ‚Äúhello‚Äù\n",
        "\n",
        "    * bigrams: pairs of words like ‚Äúgood morning‚Äù\n",
        "\n",
        "  * This helps capture small phrases as well as single words.\n",
        "\n",
        "* Fitting the vectorizer\n",
        "\n",
        "* `self.X_train = self.vectorizer.fit_transform(self.train_df[\"combined_text\"])`\n",
        "\n",
        "* `fit_transform`:\n",
        "\n",
        "    * learns a vocabulary and IDF weights from `combined_text`\n",
        "\n",
        "    * creates a sparse matrix representation of all `combined_text` rows.\n",
        "\n",
        "* `self.X_train` is a matrix where:\n",
        "\n",
        "    * each row corresponds to a training example,\n",
        "\n",
        "    * each column is a TF‚ÄìIDF feature for a specific word/phrase.\n",
        "\n",
        "* Conversation history (optional)`\n",
        "\n",
        "  * `self.history = []` is a list of `(user_utterance, bot_answer)` pairs.\n",
        "\n",
        "  * In this base version, we don‚Äôt use it for logic; it‚Äôs just there to keep a record.\n",
        "---------------------------\n",
        "### `_fallback_response`\n",
        "\n",
        "  Purpose: handle cases where the model doesn‚Äôt find a good match. This avoids giving a completely irrelevant stored answer when we‚Äôre not confident in the similarity.\n",
        "\n",
        "Steps:\n",
        "\n",
        "  * Define a list of generic responses.\n",
        "\n",
        "  * Use random.choice to return one of them.\n",
        "\n",
        "\n",
        "---------------------------\n",
        "### `get_best_match`\n",
        "* Preprocess the user input\n",
        "\n",
        "  * `processed = preprocess(user_input)`\n",
        "\n",
        "  * Cleaned version of what the user typed.\n",
        "\n",
        "* Edge case: empty text\n",
        "\n",
        "  * If after preprocessing the string is empty (e.g., user typed only punctuation), we immediately return:\n",
        "\n",
        "    * a polite message asking to rephrase\n",
        "\n",
        "    * `category = None`\n",
        "\n",
        "    * similarity `0.0`\n",
        "\n",
        "* Vectorize user input\n",
        "\n",
        "  * `user_vec = self.vectorizer.transform([processed])`\n",
        "\n",
        "  * Uses the same TF‚ÄìIDF mapping as the training data:\n",
        "\n",
        "    * creates a 1-row vector for this input.\n",
        "\n",
        "* Compute cosine similarities\n",
        "\n",
        "  * `sims = cosine_similarity(user_vec, self.X_train)[0]`\n",
        "\n",
        "  * This produces an array of similarity scores:\n",
        "\n",
        "    * length = number of training examples.\n",
        "\n",
        "    * Each value is between -1 and 1, but with TF‚ÄìIDF and positive weights, it typically ranges from 0 to 1.\n",
        "\n",
        "  * Higher score = more similar.\n",
        "\n",
        "* Find best match\n",
        "\n",
        "  * `best_idx = int(np.argmax(sims))` gives index of highest similarity.\n",
        "\n",
        "  * `best_sim = float(sims[best_idx])` gives that highest score.\n",
        "\n",
        "* Optional: choose among top-k matches\n",
        "\n",
        "  * If top_k > 1:\n",
        "\n",
        "    * `sims.argsort()` returns indices sorted by similarity.\n",
        "\n",
        "    * `[-self.top_k:]` takes the last `top_k` indices = top `k` highest scores.\n",
        "\n",
        "    * `[::-1]` reverses them so they are in descending order.\n",
        "\n",
        "    * `random.choice(top_indices)` picks one of these top candidates.\n",
        "\n",
        "  * Else (top_k == 1), we just take the best one: chosen_idx = best_idx.\n",
        "\n",
        "This introduces small randomness to avoid repeating exactly the same answer every time for similar inputs.\n",
        "\n",
        "* Retrieve the data row\n",
        "\n",
        "  * `row = self.train_df.iloc[chosen_idx]`\n",
        "\n",
        "  * `answer = row[\"Answer\"]`\n",
        "\n",
        "  * `category = row.get(\"category\", None)` gets the category if the column exists.\n",
        "\n",
        "* Return values\n",
        "\n",
        "  * `answer`: the text that the bot should say.\n",
        "\n",
        "  * `category`: label like ‚Äúwitty‚Äù, ‚Äúfriendly‚Äù, etc. (useful for logging/debugging).\n",
        "\n",
        "  * `best_sim`: the similarity of the best match (for trust/confidence).\n",
        "\n",
        "---------------------------\n",
        "### `respond`\n",
        "\n",
        "This is what the chat loop calls directly.\n",
        "\n",
        "Steps:\n",
        "\n",
        "* Find best match\n",
        "\n",
        "  * Calls `get_best_match(user_input)`.\n",
        "\n",
        "  * Gets back `(answer, category, sim)`.\n",
        "\n",
        "* Check similarity threshold\n",
        "\n",
        "  * If `sim < self.min_sim`, then:\n",
        "\n",
        "    * We decide the match isn't trustworthy.\n",
        "\n",
        "    * Replace answer with a generic fallback using `_fallback_response`.\n",
        "\n",
        "    * Set `category` to `None` because this is no longer a specific categorized answer from the dataset.\n",
        "\n",
        "* Update conversation history\n",
        "\n",
        "  * `self.history.append((user_input, answer))`\n",
        "\n",
        "  * Appends the pair `(user_input, final_answer)`.\n",
        "\n",
        "* Return:\n",
        "\n",
        "  * Returns `answer`, `category`, and `sim` for the outer code to use (e.g. for printing).\n",
        "\n",
        "---------------------------\n",
        "### `chat_loop`\n",
        "* Initialization\n",
        "\n",
        "  * Prints a message.\n",
        "\n",
        "  * `train_df = load_data(\"train.csv\")` loads and preprocesses the data.\n",
        "\n",
        "  * `bot = RetrievalChatbot(train_df, min_sim=0.15, top_k=3)` builds the model.\n",
        "\n",
        "  * `min_sim=0.15` means if similarity is < 0.15, we use fallback responses.\n",
        "\n",
        "  * `top_k=3` means we randomly pick among the top 3 best matches for variety.\n",
        "\n",
        "* User prompt\n",
        "\n",
        "  * Prints instructions to the user.\n",
        "\n",
        "* Infinite loop for conversation\n",
        "\n",
        "  * `user_input = input(\"You: \").strip()` reads user message from the terminal.\n",
        "\n",
        "  * If the user types exit or quit (any case), the loop breaks and the program ends.\n",
        "\n",
        "* Get bot response\n",
        "\n",
        "  * `bot_answer, category, sim = bot.respond(user_input)`\n",
        "\n",
        "  * If `sim` is high enough, this will be an answer from the dataset.\n",
        "\n",
        "  * If `sim` is too low, bot.respond will have switched to a generic fallback.\n",
        "\n",
        "* Print answer\n",
        "\n",
        "  * If category is not `None`, we print debug info:\n",
        "\n",
        "    * Bot (category, sim=0.xx): answer.\n",
        "\n",
        "    * This is helpful for understanding what kind of response it chose and how confident it was.\n",
        "\n",
        "  * Otherwise, we just print:\n",
        "\n",
        "    * Bot: answer.\n",
        "\n",
        "\n",
        "### Summary in One Sentence\n",
        "\n",
        "The code builds a retrieval chatbot that:\n",
        "\n",
        "* Reads example Q/A pairs from train.csv,\n",
        "\n",
        "* Converts them into TF‚ÄìIDF vectors,\n",
        "\n",
        "* For each new user input, finds the most similar existing Q/A pair using cosine similarity,\n",
        "\n",
        "* Returns the stored answer (or a fallback if similarity is too low),\n",
        "\n",
        "* And runs this in a simple command-line chat loop."
      ],
      "metadata": {
        "id": "qIvzNiJpirAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 3. Build retrieval model ----------\n",
        "class RetrievalChatbot:\n",
        "    def __init__(self, train_df: pd.DataFrame, min_sim: float = 0.15, top_k: int = 3):\n",
        "        \"\"\"\n",
        "        train_df must have columns: Question, Answer, category, combined_text.\n",
        "        min_sim: minimum cosine similarity to trust a retrieved answer.\n",
        "        top_k: number of top candidates to choose from (adds some variety).\n",
        "        \"\"\"\n",
        "        self.train_df = train_df.reset_index(drop=True)\n",
        "        self.min_sim = min_sim\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # Vectorize combined_text using TF‚ÄìIDF (unigrams + bigrams)\n",
        "        self.vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "        self.X_train = self.vectorizer.fit_transform(self.train_df[\"combined_text\"])\n",
        "\n",
        "        # Optionally keep conversation history\n",
        "        self.history = []  # list of (user_utterance, bot_answer)\n",
        "\n",
        "    def _fallback_response(self, user_input: str) -> str:\n",
        "        \"\"\"\n",
        "        Generic replies for when similarity is too low.\n",
        "        You can make this smarter / more varied.\n",
        "        \"\"\"\n",
        "        generic_replies = [\n",
        "            \"I'm not completely sure how to answer that, but I'm listening.\",\n",
        "            \"Interesting! Tell me more about that.\",\n",
        "            \"That's a good question. What do you think?\",\n",
        "            \"I don't have a perfect answer, but I'm here to chat!\",\n",
        "        ]\n",
        "        return random.choice(generic_replies)\n",
        "\n",
        "    def get_best_match(self, user_input: str):\n",
        "        \"\"\"\n",
        "        Returns: (answer, category, best_similarity)\n",
        "        \"\"\"\n",
        "        processed = preprocess(user_input)\n",
        "        if not processed:\n",
        "            return \"Could you rephrase that?\", None, 0.0\n",
        "\n",
        "        user_vec = self.vectorizer.transform([processed])\n",
        "        sims = cosine_similarity(user_vec, self.X_train)[0]\n",
        "\n",
        "        best_idx = int(np.argmax(sims))\n",
        "        best_sim = float(sims[best_idx])\n",
        "\n",
        "        # Choose among top_k for some diversity\n",
        "        if self.top_k > 1:\n",
        "            top_indices = sims.argsort()[-self.top_k:][::-1]\n",
        "            chosen_idx = int(random.choice(top_indices))\n",
        "        else:\n",
        "            chosen_idx = best_idx\n",
        "\n",
        "        row = self.train_df.iloc[chosen_idx]\n",
        "        answer = row[\"Answer\"]\n",
        "        category = row.get(\"category\", None)\n",
        "\n",
        "        return answer, category, best_sim\n",
        "\n",
        "    def respond(self, user_input: str) -> str:\n",
        "        \"\"\"\n",
        "        High-level response function used in the chat loop.\n",
        "        \"\"\"\n",
        "        answer, category, sim = self.get_best_match(user_input)\n",
        "\n",
        "        if sim < self.min_sim:\n",
        "            answer = self._fallback_response(user_input)\n",
        "            category = None\n",
        "\n",
        "        # store in history\n",
        "        self.history.append((user_input, answer))\n",
        "\n",
        "        # If you want to show category in debug/logging, you could print it here.\n",
        "        # For now we just return the answer.\n",
        "        return answer, category, sim\n",
        "\n",
        "\n",
        "# ---------- 4. Simple command-line chat loop ----------\n",
        "def chat_loop():\n",
        "    print(\"Loading data and building chatbot model...\")\n",
        "    train_df = load_data(\"train.csv\")\n",
        "    bot = RetrievalChatbot(train_df, min_sim=0.15, top_k=3)\n",
        "\n",
        "    print(\"Chatbot is ready! Type 'exit' or 'quit' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "            print(\"Bot: Goodbye! üëã\")\n",
        "            break\n",
        "\n",
        "        bot_answer, category, sim = bot.respond(user_input)\n",
        "        if category:\n",
        "            # optionally show the category (useful for debugging / explanation)\n",
        "            print(f\"Bot ({category}, sim={sim:.2f}): {bot_answer}\")\n",
        "        else:\n",
        "            print(f\"Bot: {bot_answer}\")"
      ],
      "metadata": {
        "id": "ZqOXDCAFU3bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chat_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDmGXwB_U_zA",
        "outputId": "3ddb018f-ad08-49ab-cd52-1a19fe62a0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data and building chatbot model...\n",
            "Chatbot is ready! Type 'exit' or 'quit' to stop.\n",
            "\n",
            "You: What is your name?\n",
            "Bot (professional, sim=0.78): I don't have a name.\n",
            "You: How old are you?\n",
            "Bot (friendly, sim=0.70): I don't really have an age. \n",
            "You: exit\n",
            "Bot: Goodbye! üëã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ed27UVCcVCPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}